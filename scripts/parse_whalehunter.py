"""
WhaleHunter HTML parser - binance-futures sayfasÄ±ndan veri Ã§ek
"""
import asyncio
import json
import re
from pathlib import Path
import sys
import os
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# Proje root'unu path'e ekle
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

from src.whalehunter.wh_client import WhaleHunterScraper
from loguru import logger

async def parse_binance_futures():
    """
    binance-futures sayfasÄ±ndan whale verilerini parse et
    """
    email = os.getenv("WHALEHUNTER_EMAIL")
    password = os.getenv("WHALEHUNTER_PASSWORD")
    
    if not email or not password:
        logger.error("âŒ .env dosyasÄ±nda WHALEHUNTER_EMAIL ve WHALEHUNTER_PASSWORD gerekli!")
        return None
    
    scraper = WhaleHunterScraper(email, password)
    
    # binance-futures sayfasÄ±nÄ± Ã§ek
    results = await scraper.scrape_whale_data()
    
    if not results:
        logger.error("âŒ Sayfa Ã§ekilemedi")
        return None
    
    futures_url = "https://whalehunterapp.com/binance-futures"
    
    if futures_url not in results:
        logger.error("âŒ binance-futures sayfasÄ± bulunamadÄ±")
        return None
    
    html = results[futures_url]['data']
    
    # BeautifulSoup ile parse et
    soup = BeautifulSoup(html, 'html.parser')
    
    logger.info("ğŸ“Š HTML parse ediliyor...")
    
    # 1. Table'larÄ± bul
    tables = soup.find_all('table')
    logger.info(f"  ğŸ”¹ {len(tables)} tablo bulundu")
    
    whale_data = []
    
    for idx, table in enumerate(tables):
        logger.info(f"\n  ğŸ“‹ Tablo {idx + 1}:")
        
        # Table headers
        headers = []
        thead = table.find('thead')
        if thead:
            header_cells = thead.find_all(['th', 'td'])
            headers = [cell.get_text(strip=True) for cell in header_cells]
            logger.info(f"     Headers: {headers}")
        
        # Table rows
        tbody = table.find('tbody')
        if tbody:
            rows = tbody.find_all('tr')
            logger.info(f"     Rows: {len(rows)}")
            
            for row_idx, row in enumerate(rows[:5]):  # Ä°lk 5 satÄ±r
                cells = row.find_all(['td', 'th'])
                row_data = [cell.get_text(strip=True) for cell in cells]
                logger.info(f"       Row {row_idx + 1}: {row_data}")
                
                # Coin/symbol ve volume data var mÄ± kontrol et
                if row_data:
                    whale_entry = {}
                    for i, header in enumerate(headers):
                        if i < len(row_data):
                            whale_entry[header] = row_data[i]
                    
                    if whale_entry:
                        whale_data.append(whale_entry)
    
    # 2. Script tag'larÄ±nda JSON data var mÄ±?
    scripts = soup.find_all('script')
    logger.info(f"\n  ğŸ”¹ {len(scripts)} script bloÄŸu bulundu")
    
    for script in scripts:
        script_text = script.string
        if script_text and ('whale' in script_text.lower() or 'volume' in script_text.lower()):
            # JSON benzeri data ara
            json_matches = re.findall(r'\{[^{}]*(?:"symbol"|"coin"|"volume")[^{}]*\}', script_text)
            if json_matches:
                logger.info(f"     âœ… JSON data bulundu: {len(json_matches)} entry")
                for match in json_matches[:3]:
                    logger.info(f"       {match[:100]}...")
    
    # 3. Data attribute'larÄ±
    data_elements = soup.find_all(attrs={'data-coin': True}) + soup.find_all(attrs={'data-symbol': True})
    if data_elements:
        logger.info(f"\n  ğŸ”¹ {len(data_elements)} data attribute bulundu")
    
    # 4. HTML'i kaydet (analiz iÃ§in)
    output_dir = Path(project_root) / "data"
    output_dir.mkdir(exist_ok=True)
    
    html_file = output_dir / "whalehunter_page.html"
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html)
    logger.info(f"\nğŸ’¾ HTML kaydedildi: {html_file}")
    
    # 5. SonuÃ§larÄ± kaydet
    if whale_data:
        output_file = output_dir / "whalehunter_data.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(whale_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… {len(whale_data)} whale entry kaydedildi: {output_file}")
        
        # Ä°lk birkaÃ§ entry'yi gÃ¶ster
        logger.info("\nğŸ“Š Ä°lk 3 whale entry:")
        for entry in whale_data[:3]:
            logger.info(f"  {entry}")
    else:
        logger.warning("\nâš ï¸  HiÃ§ whale data parse edilemedi - sayfa muhtemelen JavaScript ile yÃ¼kleniyor")
        logger.info("""
ğŸ’¡ Ã‡Ã¶zÃ¼m Ã¶nerileri:
1. Browser'da whalehunterapp.com/binance-futures'u aÃ§
2. F12 Developer Tools'u aÃ§
3. Network sekmesine git, XHR/Fetch filtrele
4. SayfayÄ± yenile
5. Hangi API endpoint'lerine istek atÄ±ldÄ±ÄŸÄ±nÄ± not et
6. O endpoint'leri doÄŸrudan kullanabiliriz

Alternatif: Selenium ile browser automation kullan
        """)
        
        logger.info(f"\nğŸ“„ Kaydedilen HTML dosyasÄ±nÄ± inceleyebilirsiniz: {html_file}")
    
    return whale_data

if __name__ == "__main__":
    asyncio.run(parse_binance_futures())
